{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "朴素贝叶斯分类最适合的场景就是文本分类、情感分析和垃圾邮件识别。其中情感分析和垃圾邮件识别都是通过文本来进行判断。从这里你能看出来，这三个场景本质上都是文本分类，这也是朴素贝叶斯最擅长的地方。所以朴素贝叶斯也常用于自然语言处理 NLP 的工具。\n",
    "\n",
    "sklearn\n",
    "- 高斯朴素贝叶斯(GaussianNB): 特征变量是连续变量，符合高斯分布，比如说人的身高，物体的长度。\n",
    "- 多项式朴素贝叶斯 (MultinomialNB):特征变量是离散变量，符合多项分布，在文档分类中特征变量体现在一个单词出现的次数，或者是单词的 TF-IDF 值等。以单词为粒度，会计算在某个文件中的具体次数.\n",
    "- 伯努利朴素贝叶斯(BernoulliNB): 特征变量是布尔变量，符合 0/1 分布，在文档分类中特征是单词是否出现。如果该单词在某文件中出现了即为 1，否则为 0.\n",
    "\n",
    "\n",
    "TF-IDF\n",
    "\n",
    "TF-IDF 是一个统计方法，用来评估某个词语对于一个文件集或文档库中的其中一份文件的重要程度。\n",
    "TF-IDF 实际上是两个词组 Term Frequency 和 Inverse Document Frequency 的总称，两者缩写为 TF 和 IDF，分别代表了词频和逆向文档频率。\n",
    "\n",
    "\n",
    "- 词频TF: 计算了一个单词在文档中出现的次数，它认为一个单词的重要性和它在文档中出现的次数呈正比。\n",
    "- 逆向文档频率 IDF: 是指一个单词在文档中的区分度。它认为一个单词出现在的文档数越少，就越能通过这个单词把该文档和其他文档区分开。IDF 越大就代表该单词的区分度越大。\n",
    "\n",
    "所以 TF-IDF 实际上是词频 TF 和逆向文档频率 IDF 的乘积。这样我们倾向于找到 TF 和 IDF 取值都高的单词作为区分，即这个单词在一个文档中出现的次数多，同时又很少出现在其他文档中。这样的单词适合用于分类.\n",
    "\n",
    "词频 $\\mathrm{TF}=\\frac{\\text { 单词出现的次数 }}{\\text { 该文档的总单词数 }}$\n",
    "\n",
    "\n",
    "逆向文档频率 IDF $=\\log \\frac{\\text { 文档总数 }}{\\text { 该单词出现的文档数 }+1}$\n",
    "\n",
    "注意：有些单词可能不会存在文档中，为了避免分母为 0，统一给单词出现的文档数都加 1。\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "小例子：\n",
    "我在这里举个例子。假设一个文件夹里一共有 10 篇文档，其中一篇文档有 1000 个单词，“this”这个单词出现 20 次，“bayes”出现了 5 次。“this”在所有文档中均出现过，而“bayes”只在 2 篇文档中出现过。我们来计算一下这两个词语的 TF-IDF 值。\n",
    "\n",
    "\n",
    "针对“this”, 计算 TF-IDF 值:\n",
    "词频 $\\mathrm{TF}=\\frac{20}{1000}=0.02$\n",
    "逆向文档频率 IDF $=\\log \\frac{10}{10+1}=-0.0414$\n",
    "所以 TF-IDF $=0.02 *(-0.0414)=-8.28 e-4$ 。\n",
    "针对“bayes”, 计算 TF-IDF 值:\n",
    "词频 $\\mathrm{TF}=\\frac{5}{1000}=0.005$\n",
    "逆向文档频率 $\\mathrm{IDF}=\\log \\frac{10}{2+1}=0.5229$\n",
    "TF-IDF $=0.005 * 0.5229=2.61 e-3$ 。\n",
    "很明显“bayes” 的 TF-IDF 值要大于“this” 的 TF-IDF 值。这就说明用“bayes”这个单词做区分 比单词“this”要好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfVectorizer(stop_words=stop_words, token_pattern=token_pattern)\n",
    "\n",
    "# 停用词:List。在分类中没有用的词\n",
    "# 过滤规则: 正则表达式，用于过滤掉一些没有意义的符号。\n",
    "\n",
    "# fit_transform(raw_documents, y=None)\n",
    "# 返回的矩阵表示每个单词在每个文档中的TF-IDF值\n",
    "\n",
    "# 向量属性\n",
    "# vocablary_: dict 词汇的对应关系\n",
    "# idf_: 向量的IDF值\n",
    "# stop_words_: set 停用词\n",
    "\n",
    "# 例子\n",
    "# 文档一： this is the bayes document;\n",
    "# 文档二： this is the second second document;\n",
    "# 文档三： and the third one;\n",
    "# 文档四： is this the document.\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "documents = [ \n",
    "            'this is the bayes document',\n",
    "            'this is the second second document',\n",
    "            'and the third one', \n",
    "            'is this the document'\n",
    "]\n",
    "\n",
    "tfidf_matrix = tfidf_vec.fit_transform(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不重复的词: ['and' 'bayes' 'document' 'is' 'one' 'second' 'the' 'third' 'this']\n"
     ]
    }
   ],
   "source": [
    "print('不重复的词:', tfidf_vec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每个单词的 ID: {'this': 8, 'is': 3, 'the': 6, 'bayes': 1, 'document': 2, 'second': 5, 'and': 0, 'third': 7, 'one': 4}\n"
     ]
    }
   ],
   "source": [
    "print('每个单词的 ID:', tfidf_vec.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每个单词的 tfidf 值:\n",
      " [[0.         0.63314609 0.40412895 0.40412895 0.         0.\n",
      "  0.33040189 0.         0.40412895]\n",
      " [0.         0.         0.27230147 0.27230147 0.         0.85322574\n",
      "  0.22262429 0.         0.27230147]\n",
      " [0.55280532 0.         0.         0.         0.55280532 0.\n",
      "  0.28847675 0.55280532 0.        ]\n",
      " [0.         0.         0.52210862 0.52210862 0.         0.\n",
      "  0.42685801 0.         0.52210862]]\n"
     ]
    }
   ],
   "source": [
    "print('每个单词的 tfidf 值:\\n', tfidf_matrix.toarray())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 基于分词的数据准备，包括分词、单词权重计算、去掉停用词；\n",
    "- 应用朴素贝叶斯分类进行分类，首先通过训练集得到朴素贝叶斯分类器，然后将分类器应用于测试集，并与实际结果做对比，最终得到测试集的分类准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对文档进行分词\n",
    "# 英文文档\n",
    "import nltk\n",
    "\n",
    "word_list = nltk.word_tokenize(text) # 分词\n",
    "nltk.pos_tag(word_list) # 词性标注\n",
    "\n",
    "\n",
    "# 中文文档\n",
    "import jieba\n",
    "word_list = jieba.cut(text) # 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载停用词表\n",
    "# 自己搜集停用词表，保存到stopwords.txt文件中\n",
    "stop_words = [line.strip() for line in open('stopwords.txt', encoding='UTF-8').readlines()]\n",
    "stop_words = [line.strip().decode('utf-8') for line in open('stopwords.txt').readlines()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算单词权重\n",
    "tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5)\n",
    "print('特征矩阵的大小:', features.shape)\n",
    "features = tf.fit_transform(train_contents)\n",
    "\n",
    "# max_df 表示文档中的最高出现率。如果一个词的出现率高于这个值，那么它很有可能是一个停用词。\n",
    "# 如果是0.5，那么一个词如果在50%的文档中都出现过，那么它就不是一个好的特征。\n",
    "# 这个值也可以是一个整数，表示一个词最多可以出现在多少个文档中。\n",
    "\n",
    "# min_df 表示文档中的最低出现率。如果一个词的出现率低于这个值，那么它很有可能是一个停用词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成朴素贝叶斯分类器\n",
    "\n",
    "# 多项式贝叶斯分类器\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB(alpha=0.001).fit(train_features, train_labels)\n",
    "\n",
    "# alpha = 1，采用+1方式, Laplace平滑\n",
    "# 0 < alpha < 1, lidstone平滑. 越小，迭代次数越多，精度越高"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  使用分类器做预测\n",
    "\n",
    "test_tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5, vocabulary=train_vocabulary)\n",
    "test_features=test_tf.fit_transform(test_contents)\n",
    "\n",
    "predicted_labels=clf.predict(test_features) # 求解所有后验概率并找出最大的那个\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算准确率\n",
    "from sklearn import metrics\n",
    "print('准确率:', metrics.accuracy_score(test_labels, predicted_labels))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datamining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
