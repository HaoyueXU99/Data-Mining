{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pagerank 值是： {'A': 0.33333396911621094, 'B': 0.22222201029459634, 'C': 0.22222201029459634, 'D': 0.22222201029459634}\n"
     ]
    }
   ],
   "source": [
    "# NetworkX\n",
    "import networkx as nx\n",
    "# 创建有向图\n",
    "# nx.Graph()\n",
    "G = nx.DiGraph() \n",
    "# 有向图之间边的关系\n",
    "edges = [(\"A\", \"B\"), (\"A\", \"C\"), (\"A\", \"D\"), (\"B\", \"A\"), (\"B\", \"D\"), (\"C\", \"A\"), (\"D\", \"B\"), (\"D\", \"C\")]\n",
    "for edge in edges:\n",
    "    G.add_edge(edge[0], edge[1])\n",
    "pagerank_list = nx.pagerank(G, alpha=1)\n",
    "print(\"pagerank 值是：\", pagerank_list)\n",
    "\n",
    "\n",
    "# G.add_node(\"A\")\n",
    "# G.add_nodes_from([\"B\",\"C\",\"D\",\"E\"])\n",
    "# G.remove_node(node)\n",
    "# G.remove_nodes_from([\"B\",\"C\",\"D\",\"E\"])\n",
    "\n",
    "# G.nodes()\n",
    "# G.number_of_nodes()\n",
    "\n",
    "# G.add_edge(\"A\"，\"B\")\n",
    "# G.add_weighted_edges_from [u,v,w]\n",
    "# G.edges()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting networkx\n",
      "  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m67.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: networkx\n",
      "Successfully installed networkx-3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: {('牛奶',): 4, ('面包',): 4, ('尿布',): 5, ('啤酒',): 3}, 2: {('啤酒', '尿布'): 3, ('尿布', '牛奶'): 4, ('尿布', '面包'): 4, ('牛奶', '面包'): 3}, 3: {('尿布', '牛奶', '面包'): 3}}\n",
      "[{啤酒} -> {尿布}, {牛奶} -> {尿布}, {面包} -> {尿布}, {牛奶, 面包} -> {尿布}]\n"
     ]
    }
   ],
   "source": [
    "from efficient_apriori import apriori\n",
    "# 设置数据集\n",
    "data = [('牛奶','面包','尿布'),\n",
    "           ('可乐','面包', '尿布', '啤酒'),\n",
    "           ('牛奶','尿布', '啤酒', '鸡蛋'),\n",
    "           ('面包', '牛奶', '尿布', '啤酒'),\n",
    "           ('面包', '牛奶', '尿布', '可乐')]\n",
    "# 挖掘频繁项集和频繁规则\n",
    "itemsets, rules = apriori(data, min_support=0.5,  min_confidence=1)\n",
    "print(itemsets)\n",
    "print(rules)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting efficient_apriori\n",
      "  Downloading efficient_apriori-2.0.3-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: efficient_apriori\n",
      "Successfully installed efficient_apriori-2.0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install efficient_apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WebDriver' object has no attribute 'find_element_by_xpath'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m request_url \u001b[39m=\u001b[39m base_url \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(start)\n\u001b[1;32m     58\u001b[0m \u001b[39m# 下载数据，并返回是否有下一页\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m flag \u001b[39m=\u001b[39m download(request_url)\n\u001b[1;32m     60\u001b[0m \u001b[39mif\u001b[39;00m flag:\n\u001b[1;32m     61\u001b[0m \tstart \u001b[39m=\u001b[39m start \u001b[39m+\u001b[39m \u001b[39m15\u001b[39m\n",
      "Cell \u001b[0;32mIn[1], line 21\u001b[0m, in \u001b[0;36mdownload\u001b[0;34m(request_url)\u001b[0m\n\u001b[1;32m     19\u001b[0m driver\u001b[39m.\u001b[39mget(request_url)\n\u001b[1;32m     20\u001b[0m time\u001b[39m.\u001b[39msleep(\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m html \u001b[39m=\u001b[39m driver\u001b[39m.\u001b[39;49mfind_element_by_xpath(\u001b[39m\"\u001b[39m\u001b[39m//*\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mget_attribute(\u001b[39m\"\u001b[39m\u001b[39mouterHTML\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m html \u001b[39m=\u001b[39m etree\u001b[39m.\u001b[39mHTML(html)\n\u001b[1;32m     23\u001b[0m \u001b[39m# 设置电影名称，导演演员 的 XPATH\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'WebDriver' object has no attribute 'find_element_by_xpath'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# 下载某个导演的电影数据集\n",
    "from efficient_apriori import apriori\n",
    "from lxml import etree\n",
    "import time\n",
    "from selenium import webdriver\n",
    "import csv\n",
    "driver = webdriver.Chrome()\n",
    "# 设置想要下载的导演 数据集\n",
    "director = u'宁浩'\n",
    "# 写 CSV 文件\n",
    "file_name = './' + director + '.csv'\n",
    "base_url = 'https://movie.douban.com/subject_search?search_text='+director+'&cat=1002&start='\n",
    "out = open(file_name,'w', newline='', encoding='utf-8-sig')\n",
    "csv_write = csv.writer(out, dialect='excel')\n",
    "flags=[]\n",
    "# 下载指定页面的数据\n",
    "def download(request_url):\n",
    "\tdriver.get(request_url)\n",
    "\ttime.sleep(1)\n",
    "\thtml = driver.find_element_by_xpath(\"//*\").get_attribute(\"outerHTML\")\n",
    "\thtml = etree.HTML(html)\n",
    "\t# 设置电影名称，导演演员 的 XPATH\n",
    "\tmovie_lists = html.xpath(\"/html/body/div[@id='wrapper']/div[@id='root']/div[1]//div[@class='item-root']/div[@class='detail']/div[@class='title']/a[@class='title-text']\")\n",
    "\tname_lists = html.xpath(\"/html/body/div[@id='wrapper']/div[@id='root']/div[1]//div[@class='item-root']/div[@class='detail']/div[@class='meta abstract_2']\")\n",
    "\t# 获取返回的数据个数\n",
    "\tnum = len(movie_lists)\n",
    "\tif num > 15: # 第一页会有 16 条数据\n",
    "\t\t# 默认第一个不是，所以需要去掉\n",
    "\t\tmovie_lists = movie_lists[1:]\n",
    "\t\tname_lists = name_lists[1:]\n",
    "\tfor (movie, name_list) in zip(movie_lists, name_lists):\n",
    "\t\t# 会存在数据为空的情况\n",
    "\t\tif name_list.text is None: \n",
    "\t\t\tcontinue\n",
    "\t\t# 显示下演员名称\n",
    "\t\tprint(name_list.text)\n",
    "\t\tnames = name_list.text.split('/')\n",
    "\t\t# 判断导演是否为指定的 director\n",
    "\t\tif names[0].strip() == director and movie.text not in flags:\n",
    "\t\t\t# 将第一个字段设置为电影名称\n",
    "\t\t\tnames[0] = movie.text\n",
    "\t\t\tflags.append(movie.text)\n",
    "\t\t\tcsv_write.writerow(names)\n",
    "\tprint('OK') # 代表这页数据下载成功\n",
    "\tprint(num)\n",
    "\tif num >= 14: # 有可能一页会有 14 个电影\n",
    "\t\t# 继续下一页\n",
    "\t\treturn True\n",
    "\telse:\n",
    "\t\t# 没有下一页\n",
    "\t\treturn False\n",
    " \n",
    "# 开始的 ID 为 0，每页增加 15\n",
    "start = 0\n",
    "while start<10000: # 最多抽取 1 万部电影\n",
    "\trequest_url = base_url + str(start)\n",
    "\t# 下载数据，并返回是否有下一页\n",
    "\tflag = download(request_url)\n",
    "\tif flag:\n",
    "\t\tstart = start + 15\n",
    "\telse:\n",
    "\t\tbreak\n",
    "out.close()\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.10.0-py3-none-any.whl (6.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3[socks]<3,>=1.26 in /Users/haoyuexu/opt/anaconda3/envs/datamining/lib/python3.9/site-packages (from selenium) (2.0.4)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.22.2-py3-none-any.whl (400 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.2/400.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.10.3-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /Users/haoyuexu/opt/anaconda3/envs/datamining/lib/python3.9/site-packages (from selenium) (2023.5.7)\n",
      "Collecting attrs>=20.1.0 (from trio~=0.17->selenium)\n",
      "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m173.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting sortedcontainers (from trio~=0.17->selenium)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: idna in /Users/haoyuexu/opt/anaconda3/envs/datamining/lib/python3.9/site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting sniffio (from trio~=0.17->selenium)\n",
      "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting exceptiongroup>=1.0.0rc9 (from trio~=0.17->selenium)\n",
      "  Downloading exceptiongroup-1.1.2-py3-none-any.whl (14 kB)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3,>=1.26->selenium)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sortedcontainers, sniffio, pysocks, h11, exceptiongroup, attrs, wsproto, outcome, trio, trio-websocket, selenium\n",
      "Successfully installed attrs-23.1.0 exceptiongroup-1.1.2 h11-0.14.0 outcome-1.2.0 pysocks-1.7.1 selenium-4.10.0 sniffio-1.3.0 sortedcontainers-2.4.0 trio-0.22.2 trio-websocket-0.10.3 wsproto-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './input/Emails.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m# 数据加载\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m emails \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m\"\u001b[39;49m\u001b[39m./input/Emails.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     11\u001b[0m \u001b[39m# 读取别名文件\u001b[39;00m\n\u001b[1;32m     12\u001b[0m file \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39m./input/Aliases.csv\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/datamining/lib/python3.9/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/datamining/lib/python3.9/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/datamining/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/datamining/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1662\u001b[0m     f,\n\u001b[1;32m   1663\u001b[0m     mode,\n\u001b[1;32m   1664\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1665\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1666\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1667\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1668\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1669\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1670\u001b[0m )\n\u001b[1;32m   1671\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/datamining/lib/python3.9/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    862\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    863\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    864\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './input/Emails.csv'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# 用PageRank挖掘希拉里邮件中的重要任务关系\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 数据加载\n",
    "emails = pd.read_csv(\"./input/Emails.csv\")\n",
    "# 读取别名文件\n",
    "file = pd.read_csv(\"./input/Aliases.csv\")\n",
    "aliases = {}\n",
    "for index, row in file.iterrows():\n",
    "    aliases[row['Alias']] = row['PersonId']\n",
    "# 读取人名文件\n",
    "file = pd.read_csv(\"./input/Persons.csv\")\n",
    "persons = {}\n",
    "for index, row in file.iterrows():\n",
    "    persons[row['Id']] = row['Name']\n",
    "\n",
    "# 针对别名进行转换        \n",
    "def unify_name(name):\n",
    "    # 姓名统一小写\n",
    "    name = str(name).lower()\n",
    "    # 去掉, 和@后面的内容\n",
    "    name = name.replace(\",\",\"\").split(\"@\")[0]\n",
    "    # 别名转换\n",
    "    if name in aliases.keys():\n",
    "        return persons[aliases[name]]\n",
    "    return name\n",
    "# 画网络图\n",
    "def show_graph(graph):\n",
    "    # 使用Spring Layout布局，类似中心放射状\n",
    "    positions=nx.spring_layout(graph)\n",
    "    # 设置网络图中的节点大小，大小与pagerank值相关，因为pagerank值很小所以需要*20000\n",
    "    nodesize = [x['pagerank']*20000 for v,x in graph.nodes(data=True)]\n",
    "    # 设置网络图中的边长度\n",
    "    edgesize = [np.sqrt(e[2]['weight']) for e in graph.edges(data=True)]\n",
    "    # 绘制节点\n",
    "    nx.draw_networkx_nodes(graph, positions, node_size=nodesize, alpha=0.4)\n",
    "    # 绘制边\n",
    "    nx.draw_networkx_edges(graph, positions, edge_size=edgesize, alpha=0.2)\n",
    "    # 绘制节点的label\n",
    "    nx.draw_networkx_labels(graph, positions, font_size=10)\n",
    "    # 输出希拉里邮件中的所有人物关系图\n",
    "    plt.show()\n",
    "\n",
    "# 将寄件人和收件人的姓名进行规范化\n",
    "emails.MetadataFrom = emails.MetadataFrom.apply(unify_name)\n",
    "emails.MetadataTo = emails.MetadataTo.apply(unify_name)\n",
    "\n",
    "# 设置遍的权重等于发邮件的次数\n",
    "edges_weights_temp = defaultdict(list)\n",
    "for row in zip(emails.MetadataFrom, emails.MetadataTo, emails.RawText):\n",
    "    temp = (row[0], row[1])\n",
    "    if temp not in edges_weights_temp:\n",
    "        edges_weights_temp[temp] = 1\n",
    "    else:\n",
    "        edges_weights_temp[temp] = edges_weights_temp[temp] + 1\n",
    "\n",
    "# 转化格式 (from, to), weight => from, to, weight\n",
    "edges_weights = [(key[0], key[1], val) for key, val in edges_weights_temp.items()]\n",
    "\n",
    "# 创建一个有向图\n",
    "graph = nx.DiGraph()\n",
    "# 设置有向图中的路径及权重(from, to, weight)\n",
    "graph.add_weighted_edges_from(edges_weights)\n",
    "# 计算每个节点（人）的PR值，并作为节点的pagerank属性\n",
    "pagerank = nx.pagerank(graph)\n",
    "# 获取每个节点的pagerank数值\n",
    "pagerank_list = {node: rank for node, rank in pagerank.items()}\n",
    "# 将pagerank数值作为节点的属性\n",
    "nx.set_node_attributes(graph, name = 'pagerank', values=pagerank_list)\n",
    "# 画网络图\n",
    "show_graph(graph)\n",
    "\n",
    "# 将完整的图谱进行精简\n",
    "# 设置PR值的阈值，筛选大于阈值的重要核心节点\n",
    "pagerank_threshold = 0.005\n",
    "# 复制一份计算好的网络图\n",
    "small_graph = graph.copy()\n",
    "# 剪掉PR值小于pagerank_threshold的节点\n",
    "for n, p_rank in graph.nodes(data=True):\n",
    "    if p_rank['pagerank'] < pagerank_threshold: \n",
    "        small_graph.remove_node(n)\n",
    "# 画网络图\n",
    "show_graph(small_graph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datamining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
